# Keep learning project #2 : k8s <!-- omit in toc -->

I played around with Minikube and EKS to understand Kubernetes. I would like to be a kind of expert about Kubernetes. My journey to be an expert about Kubernetes just started!

## History <!-- omit in toc -->

- [KubeCon 2019 Introduction to CNI, the Container Network Interface Project](#kubecon-2019-introduction-to-cni-the-container-network-interface-project)
- [7 GCP Kubernetes Best Practices videos](#7-gcp-kubernetes-best-practices-videos)
  - [Building small containers](#building-small-containers)
  - [Organizing Kubernetes with Namespace](#organizing-kubernetes-with-namespace)
  - [Kubernetes Health Checks with Readiness and Liveness Probes](#kubernetes-health-checks-with-readiness-and-liveness-probes)
  - [Setting Resource Requests and Limits in Kubernetes](#setting-resource-requests-and-limits-in-kubernetes)
  - [Terminating with Grace](#terminating-with-grace)
  - [Mapping External Services](#mapping-external-services)
  - [Upgrading your Cluster with Zero Downtime](#upgrading-your-cluster-with-zero-downtime)
- [Vitess: Sharded MySQL on Kubernetes](#vitess-sharded-mysql-on-kubernetes)
- [Kubernetes Operators Explained](#kubernetes-operators-explained)
- [KubeCon 2018 Keynote: Maturing Kubernetes Operators - Rob Szumski](#kubecon-2018-keynote-maturing-kubernetes-operators---rob-szumski)
- [KubeCon 2018 Kubernetes Design Principles: Understand the Why - Saad Ali, Google](#kubecon-2018-kubernetes-design-principles-understand-the-why---saad-ali-google)

## [KubeCon 2019 Doing Things Prometheus Canâ€™t Do with Prometheus](https://youtu.be/pRmnh8lgjsU)

ë°œí‘œìëŠ” 400 prometheus serverë¥¼ ìš´ì˜! ì™€ìš°!

High Availability Prometheus
- Prometheus is not distributed
- Sometimes the network breaks
- Sometimes queries make Prometheus sad
    - ì˜ëª» queryë¥¼ í–ˆë‹¤ê°€ëŠ” ì´ì œ ìì›ì„ ì—„ì²­ ì“°ê³  unresponsiveê°€ ë ìˆ˜ë„

High Availability Prometheus
- thanos-query, promxy fan out to fill gaps
- Decreased query performance
- Big queries fanning out
- Operational overhead

ğŸ¤” ì´ ë°œí‘œì—ì„œë„ PrometheusëŠ” ì´ì œ ditributedí•˜ì§€ ì•Šë‹¤ê³  ì„¤ëª…í•˜ê³ , HAë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•œ ë°©ë²•ê³¼ HAì— ì˜í•œ downsideë¥¼ ì„¤ëª…í•œë‹¤. 

Cardinality

- Every permutation of labels in Prometheus creates a new time series individual queries should use hundreds not thousands of time series 
- Queries that generate on thousands of time series will overload Prometheus 
- Work out your query in the Console before graphing 
- Avoid high cardinality lables

> avoid labels that have unbounded potential values and what the total number of labels that you put on a metric.

ğŸ¤” í•„ìš” ì—†ì´ labelë¥¼ ê·¸ëƒ¥ ìƒê°ì—†ì´ ë„£ë‹¤ë³´ë©´ ì´ì œ queryí•  ë•Œ ëª‡ ì²œê°œì˜ time seriesì—ì„œ ê°€ì ¸ì™€ì•¼ í•  ìˆ˜ë„ ìˆêµ¬ë‚˜.

ì‹¤ì§ˆì ìœ¼ë¡œ í•˜ë‹¤ë³´ë©´ ëª‡ì²œ time seriesë¥¼ ì‚¬ìš©í•˜ëŠ” queryê°€ ìƒê¸°ëŠ”ë°, ë°œí‘œìëŠ” several thousands í˜¹ì€ 10,000 ì´ ë˜ì–´ë„ í¬ê²Œ ë¬¸ì œê°€ ë  ê²ƒ ê°™ì§€ ì•Šë‹¤ê³  í•˜ê³ , í•˜ì§€ë§Œ 10,000ì´ ë„˜ì–´ê°€ë©´ ì´ì œ ì¡°ì¹˜ë¥¼ ì·¨í•´ì•¼ í•¨.

Effective Cardinality - protec
Leave resource headroom
--query.max-samples
--query.max-concurrency
shard on logical boundaries or federate

Long-term Metrics Storage
Prometheus 2.8 released disk-backed retention
storage.tsdb.max-block-durationì˜ ê°’ì„ ì´ì œ 5ì¼ ì´ë¼ê³  í•˜ë©´ 5ì¼ ë‹¨ìœ„ë¡œ ì´ì œ compactí•´ì„œ ë§Œë“œëŠ”ë° ì´ì œ disk storageê°€ ë¶€ì¡±í•´ì„œ ì§€ìš°ê²Œ ë˜ë©´ ì´ blockë‹¨ìœ„ë¡œ ì§€ìš°ê²Œ ëœë‹¤. 

**Mindful data and vanilla Prometheus could be all you need**

Block storage or a fast disk

Separate "long term" server

ğŸ¤” Thanos, Promxy, Cortexë“±ì„ ì‚¬ìš©í•´ì•¼ í•´ì„œ operation ë³µì¡ì„±ê³¼ ì¶”ê°€ì ì¸ ë¹„ìš©ì„ ì¶”ê°€í•´ì•¼í•  ì§€ ê³ ë¯¼í•´ë´ì•¼ í•œë‹¤. ê·¸ëƒ¥ Vanila Promethuesë¡œë„ ì¶©ë¶„í•œì§€ ê³ ë¯¼í•´ë´ì•¼ í•œë‹¤. Promethues ìì²´ê°€ ì´ì œ robustí•˜ê³  simpleí•˜ê²Œ ë§Œë“¤ì–´ì¡Œë‹¤ê³  í•˜ëŠ”ë°, HAì—†ì´ë„ ì¶©ë¶„íˆ ë¯¿ì„ë§Œ í•œê±¸ê¹Œ???

Machine learning on Metrics => PromQL has enough features for you to impress your boss

label ephemeral value => Prometheus has an official Go client library that you can use to instrument Go applications.

## [KubeCon 2019 Prometheus Deep Dive](https://youtu.be/Me-kZi4xkEs)

designed to be distributed
- minimal dependencies
  - Local disk
  - Network
- Intentionally un-coordinated distributed system
- Run Prometheus close to your targets
- Vertical sharding before horizontal sharding
  - 1000ê°œ podê°€ ìˆëŠ”ë° 500ê°œì”© ê°ê° ì„œë¹„ìŠ¤ë¥¼ êµ¬ì„±í•œë‹¤ë©´ Prometheusë¥¼ ê°ê° 500ì”© ê·¸ë£¹ì— ë„ì›Œì„œ.

> Prometheus design was came from a need where the monitoring system needed to be the most reliable thing on the network and which meant that the Prometheus itself needed to have the least number of dependencies on anything else on your network so as long as it's up and running it's got a little local disk and it can reach to the network it can monitor it.

### Q. why do I get float values for increase() when my counter goes up by integers?

Prometheus Metric type
* Counter
* Gauge
* Histogram
* Summary

ğŸ¤”interpolationì„ í•˜ëŠ”êµ¬ë‚˜. ê·¸ë¦¬ê³  scrapeë„ ì´ì œ ë‹¤ë¥¸ íƒ€ì´ë°ì— í•˜ë”ë¼ë„ ì´ì œ interpolationì„ í•˜ë‹ˆê¹ ìƒê´€ì´ ì—†ì–´ì§€ê³ . ê·¸ë¦¬ê³  scapeí•˜ëŠ”ë° ëª‡ ê°œ í¬ì¸íŠ¸ë¥¼ ëª»ê°€ì ¸ì™€ë„ ì´ì œ ì „ì²´ì ìœ¼ë¡œ ì •í™•ë„ê°€ ë§‰ ë–¨ì–´ì§€ëŠ”ê²Œ ì•„ë‹ˆê³ , total numberëŠ” ì•„ë‹ˆê¹!

### Q. how much space do I need for Prometheus?

recording rule
Recording rules allow you to precompute frequently needed or computationally expensive expressions and save their result as a new set of time series. Querying the precomputed result will then often be much faster than executing the original expression every time it is needed. This is especially useful for dashboards, which need to query the same expression repeatedly every time they refresh.

Typical formula: 1.5 bytes per sample per second

ì—¬ê¸° ë°œí‘œì—ì„œëŠ” 1700 targetì´ ìˆê³ , tagetë³„ 700ê°œì˜ metricì„ ìˆ˜ì§‘í•˜ê³ , ê·¸ë¦¬ê³  15ì´ˆ ì£¼ê¸°ë¡œ scapeì„ í•˜ê³  ì¼ë¶€ëŠ” 5ì´ˆë¡œ í•œë‹¤. ê·¸ë¦¬ê³  ë§ì€ recording rulesìˆë‹¤ê³  ê°€ì •í–ˆì„ ë•Œ, 100,000 samples / second * 1.5 bytes * 60 * 60 = 0.5GB/hour

ğŸ¤” 100,000ì€ 1700*700/15ë¡œ í•˜ê³  ì´ì œ 5ì´ˆ scapeë„ ìˆê³  ë§ì€ recording rulesë„ ìˆìœ¼ë‹ˆ 100,000ìœ¼ë¡œ ì¡ì•˜ê² ì§€?

### Q. how do I deal with multiple instances?

horizontal scaling of the data from multiple prometheus instances
* Cortex
* M3DB
* Thanos
* Others

ğŸ¤” Federation optionë„ ìƒê°í•´ë³¼ ìˆ˜ ìˆêµ¬ë‚˜.

Hierarchical federation

Hierarchical federation allows Prometheus to scale to environments with tens of data centers and millions of nodes. In this use case, the federation topology resembles a tree, with higher-level Prometheus servers collecting aggregated time series data from a larger number of subordinated servers.
For example, a setup might consist of many per-datacenter Prometheus servers that collect data in high detail (instance-level drill-down), and a set of global Prometheus servers which collect and store only aggregated data (job-level drill-down) from those local servers. This provides an aggregate global view and detailed local views.

Cross-service federation

In cross-service federation, a Prometheus server of one service is configured to scrape selected data from another service's Prometheus server to enable alerting and queries against both datasets within a single server.
For example, a cluster scheduler running multiple services might expose resource usage information (like memory and CPU usage) about service instances running on the cluster. On the other hand, a service running on that cluster will only expose application-specific service metrics. Often, these two sets of metrics are scraped by separate Prometheus servers. Using federation, the Prometheus server containing service-level metrics may pull in the cluster resource usage metrics about its specific service from the cluster Prometheus, so that both sets of metrics can be used within that server.

### Q. metricì— ë§ì€ labelë¥¼ ì„¤ì •í–ˆì„ ë•Œ storageì— ì˜í–¥ì´ ìˆëŠ”ì§€? 

prometheus inverted indexë¼ì„œ labelë¥¼ ì¶”ê°€í•˜ë”ë¼ë„ ì¶”ê°€ì ìœ¼ë¡œ ë§ì€ storageë¥¼ ì°¨ì§€í•˜ì§€ ì•ŠëŠ”ë‹¤. valueê°€ ë§ì•„ì§€ë©´ ì´ì œ scaní•˜ëŠ”ë° ë” ì˜¤ë˜ ê±¸ë¦¬ë‹ˆê¹ labelí•  ë•Œ cardinalityë¥¼ ìƒê°í•´ì•¼ê² êµ¬ë‚˜! 

### Q. Thanosë‚˜ cortex ì‚¬ìš©ê²½í—˜ì— ë¬¼ì–´ë´¤ë‹¤.

ë°œí‘œìëŠ” Thanosë¥¼ ì“°ê¸° ì „ì—ëŠ” ì´ì œ ì ì  ì»¤ì§€ë©´ì„œ prometheus serverê°€ ëŠ˜ì–´ë‚˜ê¸° ì‹œì‘í–ˆê³ , grafanaì—ì„œ ì–´ë–¤ data sourceëŠ” ì´ prometheus serverì—ì„œ ë‹¤ë¥¸ data sourceëŠ” ë‹¤ë¥¸ prometheus serverì—ì„œ ê°€ì ¸ì™€ì„œ ì´ì œ ì´ëŸ° ë°ì´í„°ë¥¼ mixingí•˜ëŠ” ê²ƒ ìƒë‹¹íˆ ê·€ì°®ì€ ì‘ì—…ì´ì—ˆë‹¤. ê·¸ë˜ì„œ thanosë¥¼ overlay proxyë¡œ queryë¥¼ í¸ë¦¬í•˜ê²Œ í–ˆë‹¤. Prometheus server ì— 6ê°œì›”ì¹˜ì˜ ë°ì´í„°ë¥¼ ë³´ê´€í•˜ëŠ”ë° Thanosì™€ ì˜ ì‘ë™í–ˆë‹¤. 

### Q. Thanos queryê°€ ëŠë¦´ ìˆ˜ê°€ ìˆëŠ”ë° cache layerë¥¼ ì“°ëŠ”ì§€? 

cortex cache layerê°€ thanosì— merge ë˜ì–´ì„œ ì‚¬ìš©ê°€ëŠ¥í•˜ë‹¤

### Q. alert managerì—ì„œ ì´ì œ opsíŒ€ì—ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ PromQLì²˜ëŸ¼ ì½”ë“œë¡œ í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ë­”ê°€ integration í•  ê³„íšì´ ìˆëŠ”ì§€? 

GUI grafanaì—ê²Œ ìš”ì²­í•˜ê³  ìˆë‹¤. PrometheusíŒ€ì—ì„œ í•  ê³„íšì´ ì—†ë‹¤.

ğŸ¤” [Jsonnet library](https://grafana.com/blog/2020/02/26/how-to-configure-grafana-as-code/)ë¡œ Grafanaë„ codeë¡œ ì°ì–´ ë‚¼ ìˆ˜ ìˆë‚˜ë³´êµ¬ë‚˜!

## [KubeCon 2019 Introduction to CNI, the Container Network Interface Project](https://youtu.be/YjjrQiJOyME)

"CNI was created as a common interface that could be used by any container runtime and and network."

Runtimeì´ CNIë¥¼ Callí•˜ëŠ”ë° ì´ì œ ADDë¡œ callí•˜ë©´ ì´ì œ Network interfaceë¥¼ ì»¨í…Œì´ë„ˆì— ì¶”ê°€í•œë‹¤. 

The CNI project has two major parts
1. The CNI specification documents
2. A set of reference and example plugins

Specification
1. A vendor-neutral specification - not just for Kubernetes
2. Also used by Mesos, CloudFoundry, podman, CRI-O
3. Defines a basic execution flow & configuration format for network operations
4. Attempts to keep things simple and backwards compatible

Configuration Format
1. JSON-based configuration
2. Both standard keys and plugin-specific ones
3. Configuration fed to plugin on stdin for each operation
4. Stored on-disk or by the runtime

Execution Flow
1. Basic commands: ADD, DEL, CHECK and VERSION
2. Plugins are executables
3. Spawned by the runtime when network operations are desired
4. Fed JSON configuration via stdin
5. Also fed container-specific data via stdin
6. Report structured result via stdout

Kubernetesì˜ ê²½ìš° ì´ì œ Kubeletì´ CNI pluginì„ separate programì„ ì‹¤í–‰í•˜ê³  ì´ì œ JSON configurationì´ ì»¨í…Œì´ë„ˆ ë°ì´í„°ë¥¼ stdinìœ¼ë¡œ ë„£ê³  ì´ì œ ê²°ê³¼ë¥¼ stdoutìœ¼ë¡œ responseí•˜ë©´ Kubeletì´ ë°›ì•„ê°„ë‹¤.

> The CNI plugin is selected by passing Kubelet the --network-plugin=cni command-line option. Kubelet reads a file from --cni-conf-dir (default /etc/cni/net.d) and uses the CNI configuration from that file to set up each podâ€™s network. The CNI configuration file must match the CNI specification, and any required CNI plugins referenced by the configuration must be present in --cni-bin-dir (default /opt/cni/bin). 
[kubernetes document](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni)

[Kubernetes-the-hard-way-aws](https://github.com/prabhatsharma/kubernetes-the-hard-way-aws/blob/master/docs/09-bootstrapping-kubernetes-workers.md)ì—ì„œ CNI Networkingì„ ì„¤ì •í•˜ëŠ” ê²ƒì„ ë³´ë©´ kubelet optionìœ¼ë¡œ `--network-plugin=cni` ì •ì˜í•œ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  `https://github.com/containernetworking/plugins/releases/download/v0.8.5/cni-plugins-linux-amd64-v0.8.5.tgz`ë¥¼ ë‹¤ìš´ ë°›ì•„ì„œ `/opt/cni/bin/` ê²½ë¡œì— ì••ì¶•ì„ í’€ì–´ì„œ ì €ì¥í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ `/etc/cni/net.d` ê²½ë¡œì— pod networkë¥¼ ì–´ë–»ê²Œ ì…‹ì—…í•  ì§€ configuration íŒŒì¼ë“¤ì„ ì •ì˜í•œë‹¤.

```
cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
{
    "cniVersion": "0.3.1",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "${POD_CIDR}"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
EOF
```

```
cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.3.1",
    "type": "loopback"
}
EOF
```

ğŸ¤” EKSì—ì„œëŠ” ì–´ë–»ê²Œ CNIê°€ ì…‹íŒ…ì´ ë ê¹Œ ê¶ê¸ˆí•´ì¡Œë‹¤.

ì¼ë‹¨ aws container-roadmap repoì— ì˜¬ë¼ì˜¨ [ì´ìŠˆ](https://github.com/aws/containers-roadmap/issues/71#issue-391916330)ë¥¼ ë³´ë©´ EKS clusterë¥¼ ìƒì„±í•  ë•Œ ìë™ìœ¼ë¡œ deamonsetìœ¼ë¡œ AWS VPC CNI Pluginì´ ê¹”ë¦¬ê²Œ ë˜ëŠ” ê²ƒ ê°™ë‹¤.

Deamonsetìœ¼ë¡œ ë„ëŠ” [AWS VPC CNI Pluginì˜ ë¦¬í¬](https://github.com/aws/amazon-vpc-cni-k8s)ë¥¼ ì‚´í´ë³´ë©´ ì¼ë‹¨ daemonset objectì˜ volume ì„¤ì •ì´ ë‹¤ìŒê³¼ ê°™ì´ ë˜ì–´ ìˆë‹¤.

```
volumes:
   - name: cni-bin-dir
      hostPath:
      path: /opt/cni/bin
   - name: cni-net-dir
      hostPath:
      path: /etc/cni/net.d
```

daemonsetìœ¼ë¡œ podê°€ ëœ° ë•Œ, `/etc/cni/net.d`ì— `10-aws.conflist`ë¥¼ ë„£ê²Œ ëœë‹¤.

amazon-vpc-cni-k8s/misc/10-aws.conflist
```
{
  "cniVersion": "0.3.1",
  "name": "aws-cni",
  "plugins": [
    {
      "name": "aws-cni",
      "type": "aws-cni",
      "vethPrefix": "__VETHPREFIX__",
      "mtu": "__MTU__"
    },
    {
      "type": "portmap",
      "capabilities": {"portMappings": true},
      "snat": true
    }
  ]
}
```

[AWS VPC CNI Plugin Proposal](https://github.com/aws/amazon-vpc-cni-k8s/issues/214#issuecomment-493543581)ë¥¼ ë³´ë©´ L-IPAM daemonì´ ëŒì•„ê°€ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. 

> The L-IPAM daemon is responsible for attaching elastic network interfaces to instances, assigning secondary IP addresses to elastic network interfaces, and maintaining a "warm pool" of IP addresses on each node for assignment to Kubernetes pods when they are scheduled.
[EKS document](https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html)

[AWS VPC CNI Plugin Repoì˜ ì´ìŠˆ](https://github.com/aws/amazon-vpc-cni-k8s/issues/214#issuecomment-493543581)ë¥¼ ë³´ë©´ì„œ GKEëŠ” Calico CNI Pluginì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. EKSëŠ” AWS VPC CNI Pluginìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— [ì¸ìŠ¤í„´ìŠ¤ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ENIì™€ ENIë‹¹ í• ë‹¹ ê°€ëŠ¥í•œ IP ìˆ«ì ì œí•œ](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html)ë§Œí¼ podë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

[2020ë…„ 2ì›” 14ì¼ì— VPC CNI Version 1.6ì´ ë¦´ë¦¬ì¦ˆ](https://aws.amazon.com/about-aws/whats-new/2020/02/amazon-eks-announces-release-of-vpc-cni-version-1-6/)ë˜ì—ˆë‹¤ê³  ì–´ë‚˜ìš´ìŠ¤ê°€ ë˜ì—ˆë„¤. VPC CNIì— ë‹¤ì–‘í•œ Configuration Variableë“¤ì´ ìˆêµ¬ë‚˜. `WARM_ENI_TARGET`, `WARM_IP_TARGET`, `MINIMUM_IP_TARGET` ë“±ë„ ìˆêµ¬ë‚˜. ë¯¸ë¦¬ podì— í• ë‹¹í•  IPë¥¼ deadmonìœ¼ë¡œ ëŒê³  ìˆëŠ” ipamì´ ë¯¸ë¦¬ í™•ë³´í•˜ê³  ìˆì„ ìˆ˜ë„ ìˆêµ¬ë‚˜. ì´ì œ `WARM_IP_TARGET`ë¥¼ 30ìœ¼ë¡œ í•˜ê³  ì´ì œ 30ê°œì˜ podê°€ ipë¥¼ í• ë‹¹ ë°›ìœ¼ë©´ ì´ì œ 30ê°œì˜ ipë¥¼ í™•ë³´í•˜ë ¤ê³  í•˜ê² ì§€. ê·¼ë° ì´ë ‡ê²Œ í•˜ë©´ ì´ì œ subnetì˜ í• ë‹¹ ê°€ëŠ¥í•œ ipë¥¼ ë¹ ë¥´ê²Œ ê³ ê°ˆ ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë‹ˆê¹ ì´ì œ `MINIMUM_IP_TARGET`ê°€ ì¶”ê°€ë˜ì–´ì„œ ì´ì œ ì´ ê°’ì„ 30ìœ¼ë¡œ í•˜ê³  `WARM_IP_TARGET`ë¥¼ 2ë¡œ í•˜ë©´ ì´ì œ 30ê°œë¥¼ í™•ë³´ë¥¼ ë™ì¼í•˜ê²Œ í•˜ì§€ë§Œ ì´ì œ 30ê°œê°€ podê°€ deployë˜ì„œ í• ë‹¹ë˜ê³  ë‚˜ë©´ ì´ì œ ipam daemonëŠ” 2ê°œë§Œ ë¯¸ë¦¬ ipë¥¼ í™•ë³´ í•´ë‘”ë‹¤.

## 7 GCP Kubernetes Best Practices videos

### [Building small containers](https://youtu.be/wGz_cbtCiEA)

Performance ì¸¡ë©´ì—ì„œ small containerê°€ build, push, pullí•˜ëŠ”ë° ìœ ë¦¬í•˜ë‹¤. ì—¬ê¸°ì„œëŠ” Google container registry ì„œë¹„ìŠ¤ì—ì„œ base imageë¥¼ cacheí•˜ê³  ìˆê¸° ë•Œë¬¸ì— pushì—ì„œëŠ” pushí•˜ëŠ” timeì´ í¬ê²Œ ì°¨ì´ ì•ˆë‚œë‹¤ê³  í•œë‹¤.

ê¸°ë³¸ì ìœ¼ë¡œ container ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•˜ê³  ìˆì—ˆë˜ alpine base image ì‚¬ìš©í•˜ê¸°ì™€ multi stage buildë¥¼ ì„¤ëª…í•˜ê³  ìˆë‹¤.

ê·¸ë¦¬ê³  small containerì¼ ìˆ˜ë¡ ë³´ì•ˆì ìœ¼ë¡œ ë…¸ì¶œ ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ì ë‹¤. ì—¬ê¸°ì„œëŠ” Container Registry Vulnerability Scanning serviceë¡œ go:onbuildì˜ containerì™€ multi staged buildì˜ imageë¥¼ scaní•´ì„œ vulnerabilityê°€ í° ì‚¬ì´ì¦ˆì˜ ì»¨í…Œì´ë„ˆê°€ ë” ë§ì€ ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.

[AWS ECRì—ì„œë„ 2019ë…„ 10ì›”ì— Image scanning ê¸°ëŠ¥ì„ ì¶œì‹œí–ˆêµ¬ë‚˜.](https://aws.amazon.com/about-aws/whats-new/2019/10/announcing-image-scanning-for-amazon-ecr/)

### [Organizing Kubernetes with Namespace](https://youtu.be/xpnZX3if9Tc)

ê¸°ë³¸ì ìœ¼ë¡œ ìƒê¸°ëŠ” Kubernetes
- default
- kube-system
- kube-public

active namespaceë¥¼ í¸ë¦¬í•˜ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” Tool
- kubens: switch your active namespace to the namespace you want
- ê·€ì°®ê²Œ kubectl get pods --namespace=something ì²˜ëŸ¼ namespace optionì„ ì§€ì •í•˜ëŠ” ëŒ€ì‹ ì— kubensë¡œ active namespaceë¥¼ ë°”ê¿”ì„œ ê´€ë¦¬

Cross Namespace Communication
- Services in Kubernetes expose their endpoint using a common DNS pattern \
  `<Service Name>.<Namespace Name>.svc.cluster.local`
- ì´ì œ ê·¸ëƒ¥ `servicename`ìœ¼ë¡œ í•˜ê±°ë‚˜ ê°™ì€ namespaceì— ë™ì¼í•œ ì´ë¦„ì˜ ì„œë¹„ìŠ¤ê°€ ìˆë‹¤ë©´ `servicename.namespacename`ìœ¼ë¡œ!

ì–´ë–»ê²Œ namespaceë¥¼ managableí•˜ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆì„ê¹Œ?

ğŸ¤” ê²°êµ­ì€ namespaceë„ ì–´ëŠì •ë„ì˜ isolationì„ íŒ€ì—ê²Œ ì¤„ ê±´ì§€ì— ë”°ë¼ì„œ ê²°ì •ë˜ê² ì§€. ì•„ì£¼ ì‘ì€ íŒ€ì—ì„œëŠ” ê·¸ëƒ¥ default namespaceë¥¼ ì“°ëŠ” ê²ƒì´ ì¶©ë¶„í•  ìˆ˜ ìˆê³ , íŒ€ì´ ë” ì»¤ì§€ê²Œ ë˜ë©´ ì´ì œ ì„œë¡œì˜ íŒ€ë“¤ì´ ë…ë¦½ì„±ì„ ì¤„ ìˆ˜ ìˆë„ë¡ namespaceë¥¼ êµ¬ë¶„í•˜ê±°ë‚˜ ì´ì œ ì •ë§ ê·¸ëƒ¥ APIë¡œ ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ í†µì‹ í•˜ë©´ ë˜ë©´ ì´ì œ clusterë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ê²ƒì´ê² ì§€. Monolithic application vs microservice architecture ì¤‘ì— ì¡°ì§ì—ì„œ ì–´ë–¤ ê²ƒì´ í•„ìš”í• ì§€ ê²°ì •í•˜ëŠ” ê²ƒê³¼ Namespace ê´€ë¦¬ê°€ ë¹„ìŠ·í•œ ì´ìŠˆì¸ê²ƒ ê°™ë‹¤.

### [Kubernetes Health Checks with Readiness and Liveness Probes](https://youtu.be/mxEvAPQRwhw)

Types of Health Checks
1. Readiness
   - by default, Kubernetes will start sending traffic as soon as the process inside the container start.
2. Liveness
   - restart a pod

Types of Probes
1. HTTP: 200ëŒ€ response statusë°›ìœ¼ë©´ success
2. Command: exit statusê°€ zeroì´ë©´ success
3. TCP: connection establishí•˜ë©´ success

Configuring Probes
- initialDelaySeconds
  - P99 startup time or average time with buffer
- periodSeconds
- timeoutSeconds
- successThreshold
- failureTrhreshold

### [Setting Resource Requests and Limits in Kubernetes](https://youtu.be/xjpHggHKm78)

Requests and Limits

cpuì˜ ê²½ìš° limitì„ ë„˜ì–´ê°€ë ¤ë©´ restrictí•´ì„œ performanceê°€ ì•ˆ ì¢‹ì•„ì§€ì§€ë§Œ ê³„ì† ì‹¤í–‰ëœë‹¤. í•˜ì§€ë§Œ memoryê°™ì€ ê²½ìš° limitì„ ë„˜ì–´ê°€ë©´ ì´ì œ ê·¸ containerëŠ” terminateëœë‹¤.

~~resource requestê°€ ì§€ê¸ˆ nodeê°€ ì‚¬ìš©ê°€ëŠ¥í•œ resourceë¥¼ ë„˜ì–´ê°€ê²Œ ë˜ë©´ ì´ì œ ê·¸ pod pending ìƒíƒœê°€ ë˜ê³  ì´ì œ pendingì¸ podë³´ë‹¤ priorityê°€ ë‚®ì€ podê°€ ì´ì œ evictë˜ê³  queueì—ì„œ ê¸°ë‹¤ë¦¬ëŠ” priorityê°€ ë†’ì€ podê°€ schedule ëœë‹¤.~~ request resourceë‘ ì‹¤ì œë¡œ nodeì—ì„œ ì‚¬ìš©ì¤‘ì¸ resourceëŠ” ë‹¤ë¥´ë‹¤! ê·¸ë˜ì„œ pending stateëŠ” ì´ì œ requests resourceí™•ë³´ê°€ ë…¸ë“œì—ì„œ ì•ˆë˜ë‹ˆê¹ pendingìƒíƒœê°€ ë˜ëŠ” ê±°ê³  ì´ì œ ì´ ìƒíƒœì—ì„œëŠ” ë”°ë¡œ runningì¤‘ì¸ podë¥¼ killí•  í•„ìš” ì—†ê² ì§€. ê·¼ë° ì´ì œ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ë¦¬ì†ŒìŠ¤ê°€ ì´ì œ requestsë¥¼ ë„˜ì–´ì„œ limitê¹Œì§€ ê°€ê³  ì´ì œ nodeì „ì²´ë¡œ ë´¤ì„ ë•Œ nodeì˜ resourceë¥¼ ë„˜ì–´ê°€ê²Œ ë˜ë©´ ì´ì œ KubernetesëŠ” ì´ì œ ìì› í™•ë³´ë¥¼ ìœ„í•´ì„œ podë¥¼ killí•´ì•¼ê² ì§€!

nodeë¡œ ì‚¬ìš©ë˜ëŠ” EC2 instanceì˜ vCPUê°€ 2ì¸ë°, request cpuë¥¼ 2.5 CPUë¡œ í•œë‹¤ê³  í•˜ë©´ ì´ podëŠ” ê³„ì† ì‹¤í–‰ë  ìˆ˜ ì—†ì„ ê²ƒì´ë‹¤. \
ğŸ¤” ì´ëŸ° ê²½ìš°ì— evictëŠ” ì–´ë–»ê²Œ ë˜ëŠ”ê±°ì§€? => evictëŠ” ì•ˆë˜ê³  ì´ì œ pending stateë¡œ ë‚¨ì•„ìˆê² ì§€!

ì´ì œ namespaceì—ì„œë„ ResourceQuotaë‘ LimitRangeë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. `kind: ResourceQuota`ë¡œ ì´ì œ `requests.cpu`, `request.memory`, `limits.cpu`, `limits.memory`ë¥¼ ì„¤ì •í•´ì„œ namespaceì˜ containerë“¤ì˜ í•©ì´ ì´ê²ƒë“¤ì„ ë„˜ì§€ ì•Šë„ë¡ í•  ìˆ˜ ìˆë‹¤. `kind: LimitRange`ëŠ” ì´ì œ ì „ì²´ namespaceê°€ ì•„ë‹ˆë¼ ê°ê°ì˜ containerì— default, limit, max, minë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. defaultë¥¼ ì„¤ì •ì•ˆí•˜ê³  maxë§Œ í•œë‹¤ë©´ ì´ì œ defaultê°’ì´ maxê°€ ëœë‹¤. defaultë¥¼ ì§€ì •ì•ˆí•˜ê³  minê°€ ìˆë‹¤ë©´ ì´ì œ defaultê°’ì€ minê°’ì´ ëœë‹¤. 

GKEì˜ auto scalerëŠ” ì´ì œ nodeê°€ requestsë¥¼ ë§Œì¡±í•  ìˆ˜ ì—†ì–´ì„œ ì´ì œ pending stateê°€ ëœ podê°€ ìˆìœ¼ë©´ ì´ì œ nodeë¥¼ ë” ì¶”ê°€í•´ì„œ ê·¸ podë¥¼ ì‹¤í–‰í•œë‹¤. \
ğŸ¤” pending stateê°€ ìˆë‹¤ê³  ë¬´ì¡°ê±´ evictë˜ëŠ” ê²ƒ ê°™ì§€ëŠ” ì•Šë‹¤? pending stateì¸ë° prorityê°€ ë‚®ì€ running podê°€ ìˆì„ ë•Œë§Œ evictê°€ ë˜ëŠ”ê±´ê°€? -> ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” resource memoryê°€ node memoryë¥¼ ë„˜ì–´ê°€ë©´ ì´ì œ evictê°€ ì§„í–‰ë˜ëŠ”ê±°ì§€!

Overcommitment

ì´ì œ requestsê°€ ìˆê³  Limitì´ ìˆëŠ”ë°, podê°€ requestsë³´ë‹¤ Limitê¹Œì§€ ë” ë§ì´ ì“¸ ìˆ˜ ìˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ì´ì œ Nodeê°€ ê°€ì§€ê³  ìˆëŠ” ë¦¬ì†ŒìŠ¤ë³´ë‹¤ ë” ë§ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. CPUê°™ì€ ê²½ìš°ëŠ” ì´ì œ compressí•´ì„œ ì„±ëŠ¥ì´ ëŠë ¤ì§€ì§€ë§Œ ì œí•œí•´ì„œ ê³„ì† ì‘ì—…ì„ í•  ìˆ˜ ìˆëŠ”ë°, ì´ì œ memoryê°™ì€ ê²½ìš°ì—ëŠ” Out Of Memoryë¡œ ì´ì œ ì „ì²´ ì‹œìŠ¤í…œì´ ë‹¤ìš´ë  ìˆ˜ ìˆë‹¤. ì´ì œ ë…¸ë“œì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ë„˜ì–´ê°€ê²Œ ë˜ë©´ ì´ì œ overcommitted stateê°€ ë˜ê³  ì´ì œ Kubernetesê°€ resourceë¥¼ í™•ë³´í•˜ê¸° ìœ„í•´ì„œ ì–´ë–¤ podë¥¼ terminateí• ì§€ ê²°ì •í•´ì•¼ í•œë‹¤. ì´ ê²°ì •ì— ìˆì–´ì„œ podì˜ priorityì— ë”°ë¼ì„œ ê²°ì •ë˜ê³  ê°™ì€ priorityë¼ê³  í•˜ë©´ ì´ì œ requests resourceë³´ë‹¤ ë” ë§ì´ ì‚¬ìš©í•˜ê³  ìˆëŠ” podê°€ terminateëœë‹¤. 

Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to:

```
1 AWS vCPU
1 GCP Core
1 Azure vCore
1 IBM vCPU
1 Hyperthread on a bare-metal Intel processor with Hyperthreading
```

### [Terminating with Grace](https://youtu.be/Z_l_kE1MDTc)

It's important that your application can handle termination gracefully(need to hand SIGTERM message)

Kubernetes Termination Lifecycle
1. Pod in Terminating State
2. The preStop Hook is excueted
   - If you application doesn't gracefully shut down when receiving a sigterm, you can use the preStop hook to trigger a graceful shutdown.
3. SIGTERM signal sent to pod
4. terminationGracePeriodSecondsì•ˆì— ì´ì œ containerê°€ ì¢…ë£Œë˜ë©´ ì´ì œ ë‹¤ìŒ stepì´ ì§„í–‰ë˜ê³ , ê·¼ë° ì•„ì§ë„ containerê°€ runningì¤‘ì´ë©´ SIGKILL ë©”ì„¸ì§€ë¥¼ ë³´ë‚¸ë‹¤. (terminationGracePeriodSecondsëŠ” preStop Hookê³¼ SIGTERMì„ ì²˜ë¦¬í•˜ëŠ” ê²ƒê³¼ parellelí•˜ê²Œ countëœë‹¤)

### [Mapping External Services](https://youtu.be/fvpq4jqtuZ8)

`kind: Service`ë¥¼ `type: ClusterIP`ë¡œ ë§Œë“¤ì–´ì„œ ì´ì œ ì—¬ê¸°ì„œ ì •ì˜í•œ nameìœ¼ë¡œ ê°€ë¦¬í‚¬ ìˆ˜ ìˆë„ë¡ í•˜ê³ , `kind: Endpoints`ì— ì´ì œ ip addressì™€ portë¥¼ ì •ì˜í•´ì„œ ì´ì œ ì´ìª½ìœ¼ë¡œ requestê°€ ê°€ë„ë¡ í•œë‹¤.

Kubernetesì•ˆì—ì„œ MongoDBê°€ ëŒê³  ìˆëŠ”ê²Œ ì•„ë‹ˆë¼ ì´ì œ Virtual Machineì—ì„œ ë³„ë„ë¡œ MongoDBê°€ ëŒê³  ìˆë‹¤ê³  í•  ë•Œ Kubernetesì˜ serviceì²˜ëŸ¼ ì´ì œ ì´ MongoDBë¥¼ ì—°ê²°í•  ìˆ˜ ìˆëŠ” ê²ƒ. ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤í„°ì•ˆì—ëŠ” ì—†ì§€ë§Œ ì´ì œ podì—ì„œ ì´ì œ mongdbë¼ëŠ” ì„œë¹„ìŠ¤ ì´ë¦„ìœ¼ë¡œ ìš”ì²­í•  ìˆ˜ ìˆê²Œ ë˜ëŠ” ê²ƒ.

ì´ì œ virtual machineì´ ê°™ì€ VPCì— ìˆê³  private IP addressë¥¼ ì´ì œ Endpoints serviceì— ë“±ë¡í•´ì„œ ì—°ê²°í•  ìˆ˜ ìˆê² ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ Databaseë‚˜ ê·¸ëŸ°ê²ƒë“¤ì´ DNSë¥¼ ì œê³µí•œë‹¤. ê·¸ëŸ´ ë•ŒëŠ” ì´ì œ `kind: Service`ë¥¼ `type: ExternalName`ë¡œ ë§Œë“¤ì–´ì„œ í•´ë‹¹ DNSë¡œ redirectí•  ìˆ˜ ìˆë‹¤. This service will do a simple CNAME redirect at the kernel level so there's very minimal impact on your performance. ê·¼ë° ì´ ë°©ë²•ì€ portê°€ staticí•˜ê²Œ ë˜ì–´ ìˆìœ¼ë©´ applicationì—ì„œ ë°”ê¿€ í•„ìš”ê°€ ì—†ëŠ”ë°, ì´ì œ ì˜ìƒì˜ ì˜ˆì œì²˜ëŸ¼ testì™€ prodì˜ MongoDB instanceê°€ ë‹¤ë¥¸ portê°€ ì„¤ì •ë˜ê³  ì´ê²Œ dynamicí•˜ê²Œ ì„¤ì •ëœë‹¤ê³  í•˜ë©´ í•œê³„ì ì´ ìˆë‹¤. ì´ì œ IPê°€ ë³€ê²½ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•˜ë©´ ì´ì œ ì£¼ì–´ì§„ DNS lookupì„ í•´ì„œ ipë“¤ì„ Endpoints ì„œë¹„ìŠ¤ì— ì ìš©í•´ì„œ í•  ìˆ˜ ìˆê² ì§€ë§Œ, ë‚´ê°€ ê²½í—˜í•œ ìƒí™©ì—ì„œëŠ” ì´ëŸ¬í•œ IPë“¤ì´ ì•ˆë°”ë€ë‹¤ëŠ” ê²ƒì„ ë³´ì¥í•  ìˆ˜ ì—†ì–´ì„œ í™œìš©í•˜ê¸° í˜ë“¤ ê²ƒ ê°™ë‹¤.

ğŸ¤” ì´ì œ ì¿ ë²„ë„¤í‹°ìŠ¤ í´ëŸ¬ìŠ¤íŠ¸ë¥¼ ìš´ì˜í•˜ê³  ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ RDSê°™ì€ ê²ƒì„ ì‚¬ìš©í•œë‹¤ê³  í•˜ë©´ ExternalName service typeì„ ì‚¬ìš©í•´ì„œ DNS redirectí•˜ëŠ” ê²ƒë„ ìƒê°í•´ë³¼ ìˆ˜ ìˆê² ë‹¤. ê·¼ë° ê·¸ëƒ¥ í™˜ê²½ë³€ìˆ˜ë¡œ ê·¸ëƒ¥ database DNSë¥¼ í™˜ê²½ë³„ë¡œ ê·¸ëƒ¥ ì£¼ì…í•´ì„œ ì—°ê²°í•˜ëŠ” ê²ƒë³´ë‹¤ ì¥ì ì´ ìˆëŠ” ê±¸ê¹Œ???

### [Upgrading your Cluster with Zero Downtime](https://youtu.be/ajbC1yTW2x0)

GKEë¡œ zero downtime upgrade ì„¤ëª…

Upgrading Nodes with Zero Downtime
1. Rolling Update
2. Migration with Node Pools

ì§€ê¸ˆ ì´ ê¸€ì„ ì‘ì„±í•˜ëŠ” 2020ë…„ 2ì›” 12ì¼ ê¸°ì¤€ìœ¼ë¡œ GKEëŠ”
- Stable channel: 1.14.10-gke.17
- Regular channel: 1.15.7-gke.23

AWS EKS Kubernetes versions
- 1.14.9
- 1.13.12
- 1.12.10

2019ë…„ 10ì›” 4ì¼ì— [Amazon EKS now supports Kubernetes version 1.14](https://aws.amazon.com/about-aws/whats-new/2019/09/amazon-eks-now-supports-kubernetes-version-1-14/)ê°€ ê³µì§€ë˜ì—ˆë„¤. 

## [Vitess: Sharded MySQL on Kubernetes](https://youtu.be/E6H4bgJ3Z6c)

ğŸ¤” Youtubeì—ì„œ ì‚¬ìš©í•˜ì˜€ê³  Borgë•Œë¶€í„° stateless appìœ¼ë¡œ ì ìš©ë˜ì–´ k8sì— ë°”ë¡œ í™œìš© ë  ìˆ˜ ìˆì—ˆêµ¬ë‚˜! \
ğŸ¤” ì´ ì˜ìƒì—ì„œëŠ” Kubernetesìœ„ì—ì„œ MySQLë¥¼ Vitessë¡œ ìš´ì˜í•˜ëŠ” ê²ƒì„ ì„¤ëª…í–ˆëŠ”ë°, major adoptorì¸ Slackì€ 2019 ìµœê·¼ Kubeconì—ì„œ ì•„ì§ Kubernetesìœ„ì— ìš´ì˜í•˜ì§€ ì•Šê³  EC2ìœ„ì— ìš´ì˜í•˜ê³  ìˆë‹¤ê³  í–ˆë‹¤. ê·¸ë¦¬ê³  ë‹¤ë¥¸ ìš°ì„ ìˆœìœ„ê°€ ìˆì–´ì„œ ê³„ì† EC2ì— migrationì„ í•œë‹¤ê³  í–ˆë‹¤. \
ğŸ¤” ì œì¼ ê¶ê¸ˆí–ˆë˜ ê²ƒì´ì—ˆëŠ”ë° ì˜ìƒ ë§ˆì§€ë§‰ Q&Aì‹œê°„ì— ì§ˆë¬¸ì´ ìˆì—ˆë‹¤. Vitessê°€ AWS MySQLê³¼ ë¹„êµí•´ì„œ ì¥ì ì„ ë¬¼ì—ˆëŠ”ë°, ì œí•œ ì—†ì´ scaleì´ ê°€ëŠ¥í•˜ê³  instance sizeë¥¼ ì„ íƒí•  í•„ìš”ì—†ì´ ë” ë”± ë§ëŠ” ì‚¬ì´ì¦ˆë¥¼ ìš´ì˜í•  ìˆ˜ ìˆê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ AWSì— ê°‡í˜€ ìˆì§€ ì•Šê³  migrationë¥¼ í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆë‹¤ê³  ì„¤ëª…í•œë‹¤. Vitessë¥¼ ë³´ë©´ì„œ Kubernetesë¥¼ ì§„ì§œ ë©‹ìˆê²Œ ì“°ë ¤ë©´ Databaseë„ Kubernetesì— ì˜¬ë¦¬ê³  íŠ¹ì • Cloud service vendorì— ê°–íˆì§€ ì•Šê³  ë¹„êµì  ì‰½ê²Œ migrationí•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ ìƒê°í–ˆëŠ”ë°, ì´ ì˜ìƒì—ì„œ ê·¸ ë¶€ë¶„ì„ ë‹¤ì‹œ í•œë²ˆ ê°•í•˜ê²Œ ë•Œë¦° ê²ƒ ê°™ë‹¤. \
ğŸ¤” Demoì—ì„œ Materialized viewë¥¼ ìƒì„±í•˜ì—¬ì„œ í™œìš©í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤¬ë‹¤. Vitessë¡œ ì‰½ê²Œ Materialized viewë¥¼ ìƒì„±í•˜ë„¤? MySQLì—ì„œ ë‚´ë¶€ì ìœ¼ë¡œ ì‰½ê²Œ Materialized viewë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆëŠ”ê±´ê°€? \
ğŸ¤” mysql proxyê°€ postgresql proxyëŠ” shardingë˜ì–´ ìˆëŠ” ê²½ìš° unified viewë¥¼ ì œê³µí•˜ì—¬ aggregationì„ í•´ì£¼ë‚˜?

> If you instead use a storage that is provided by the cloud like RDS, then migration out of that is a lot harder. Whereas if you are 100% on kubernetes it is so easy to move.

What is Vitess
- Sharding middleware for MySQL
- Massively scalable
- HA
- Cloud-native

Key adopters
- Slack
- Square
- Pinterest

Architecture

vtgateê°€ stateless app serverë¡œ clientë¡œë¶€í„°ì˜ query requestë¥¼ ë°›ëŠ”ë‹¤.
vttabletì´ ê°œë³„ì ìœ¼ë¡œ mysql instanceë¥¼ ì‹¤í–‰í•˜ê³  ê´€ë¦¬í•œë‹¤. vttabletì´ ëœ¨ë©´ ì´ì œ topologyì— ìŠ¤ìŠ¤ë¡œ ë“±ë¡í•˜ê³  vtgateê°€ discoveryí•œë‹¤.

stateful setê°€ í•˜ë‚˜ì˜ instanceë§Œ masterë¡œ ì§€ì •í•˜ëŠ” ê²ƒì„ í—ˆìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ì œ pod typeë¥¼ masterì™€ slaveë¡œ êµ¬ë¶„í•´ì„œ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ê·¸ëŸ°ë° ì´ì œ masterê°€ shut downë˜ë©´ ì´ì œ slaveë¡œ ì„¤ì •ë˜ì–´ ìˆë˜ ê²ƒì„ pod typeë¥¼ ë°”ê¿”ì„œ masterë¡œ ìŠ¹ê²© ì‹œí‚¬ ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì— master podê°€ ë‹¤ì‹œ ì‹¤í–‰ë˜ì–´ì„œ trafficì„ ë°›ì„ ë•Œê°€ì§€ ê¸°ë‹¤ë ¤ì•¼ ëœë‹¤. ì´ ë°©ë²•ì€ HAì— ì í•©í•˜ì§€ ì•Šë‹¤. ê·¸ë˜ì„œ vtgateê°€ masterë¥¼ ì²´í¬í•´ì„œ ë¬¸ì œê°€ ìˆìœ¼ë©´ ì´ì œ replicaë¡œ ìŠ¤ìœ„ì¹­í•´ì„œ ì‘ì—…ì„ ì™„ë£Œí•˜ê³  í´ë¼ì´ì–¸íŠ¸ëŠ” ì—ëŸ¬ë¥¼ ë¦¬í„´ ë°›ëŠ”ê²Œ ì•„ë‹ˆë¼ 1~2ì´ˆ latencyê°€ ëŠ˜ì–´ë‚˜ëŠ” ê²ƒì„ ê²ªê²Œ ëœë‹¤.

MySQL ê°™ì€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ë¥¼ ì“°ëŠ” ê²ƒì´ ì„±ëŠ¥ìƒ ìœ ë¦¬í•˜ë‹¤. Kubernetesì—ì„œëŠ” ì´ì œ podë¥¼ shut downí•˜ë©´ dataë¥¼ ë‹¤ ì§€ìš°ê²Œ ëœë‹¤. ë³´í†µ EBSë‚˜ Container Storage Interface baseì˜ storageë¥¼ ë¶™ì—¬ì„œ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ”ë°, local pv ê¸°ëŠ¥ì€ ì œí•œì´ ìˆì–´ì„œ ì•„ì§ í”„ë¡œë•ì…˜ì—ì„œ ë§ì´ í™œìš©ë˜ì§€ ëª»í•˜ëŠ” ê²ƒ ê°™ë‹¤. Vitess architectureì—ì„œëŠ” podê°€ shutdownë˜ë©´ ì´ì œ backupìœ¼ë¡œ ë¶€í„° restoreí•˜ê³  ê·¸ë‹¤ìŒì— catch upì„ í•œë‹¤ìŒì— íŠ¸ë˜í”½ì„ ë‹¤ì‹œ ë°›ê²Œ ëœë‹¤. semisynchronous replication featureì„ ì´ìš©í•´ì„œ slaveì—ì„œ ackë¥¼ ë°›ì•„ì•¼ í•˜ë„ë¡ í•´ì„œ replicaì— transactionì´ ë³´ê´€ëœ ê²ƒì„ ë³´ì¥í•  ìˆ˜ ìˆë‹¤. masterì™€ replicaê°€ ë™ì‹œì— ë‹¤ shoutdownë˜ì§€ ì•Šìœ¼ë©´ ì´ì œ data lossëŠ” ë°©ì§€í•˜ê²Œ ëœë‹¤.

Vitessì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ë“¤ì„ í†µí•´ì„œ ì¥ì ì„ ê°€ì ¸ê°ˆ ìˆ˜ ìˆë‹¤.
1. main db í•˜ë‚˜ ìˆì„ ë•Œ
   - Connection pooling
   - Deadlines
   - Hot row protection
   - Row count limit
2. Master & replicas
   - Replica routing
   - Load balancing
   - Master promotion with Orchestrator
3. Shards
   - Unified view
   - Sharding agnostics

## [Kubernetes Operators Explained](https://youtu.be/i9V4oCa5f9I)

ğŸ˜‰ OperatorëŠ” ì´ì œ Abstractioní•´ì„œ clientê°€ single YAML fileë¡œ createí•˜ë©´ Operatorê°€ ë°›ì•„ì„œ ë³µì¡í•œ ìš”ì†Œë“¤ì„ ë§Œë“¤ì–´ ì¤„ ìˆ˜ ìˆêµ¬ë‚˜.

Control loop
- Observe
- Diff
- Act

Controller acts on that for every default resource

In cluster, you need
- Operator Lifecycle Manager
- Operator

Two major components of operator
- CRD
- Controller

## [KubeCon 2018 Keynote: Maturing Kubernetes Operators - Rob Szumski](https://youtu.be/kld1Fi8RrRQ)

ğŸ¤” Databaseë¥¼ k8sìœ„ì—ì„œ ì œê³µí•˜ëŠ” ê²ƒì´ startupì—ì„œë„ ê°€ëŠ¥í• ê¹Œ?
- Communityì—ì„œ ë§Œë“  Operatorë¥¼ ì‚¬ìš©í•˜ì—¬ MySQLì´ë‚˜ Redisë¥¼ k8sìœ„ì— ì˜¬ë ¤ì„œ ì‚¬ìš©í•˜ê¸° ì‰¬ìš¸ê¹Œ? stableí•˜ê³  ì•ˆì „í• ê¹Œ?

Kubernetes adoption phases
1. Stateless apps
   - ReplicaSets
   - Deployments
2. Stateful apps
   - StatefulSets
   - Storage/CSI
3. Distributed systems
   - Data rebalancing
   - Autoscaling
   - Seamless upgrades

Operator framework
- Operator SDK: build
- Operator Lifecycle manager: run 
- Operator metering: operate

## [KubeCon 2018 Kubernetes Design Principles: Understand the Why - Saad Ali, Google](https://youtu.be/ZuIQurh_kDk)

ğŸ˜™ Kubernetesì˜ ì£¼ìš” Principleë“¤ì„ ì´í•´í•  ìˆ˜ ìˆì—ˆë‹¤.
- imperative ëŒ€ì‹ ì— declarativeë¡œ êµ¬ì„±í•˜ì˜€ì„ ë•Œ ì–´ë–¤ ì¥ì ì´ ìˆëŠ”ì§€?
- ì™œ ê° componentë“¤ì´ API serverë¥¼ watchí•˜ì—¬ ì‘ë™í•˜ëŠ”ì§€?
- hidden internal APIì—†ì´ Kubernetesì˜ ëª¨ë“  componentë“¤ì´ ê°™ì€ APIë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì»¤ìŠ¤í…€í•˜ê²Œ component ì§ì ‘ êµ¬ì„±í•˜ì—¬ ëŒ€ì²´í•˜ê±°ë‚˜ í™•ì¥í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•´ì§€ëŠ” ê²ƒ.
- secret, config mapë“±ì—ì„œ dataë¥¼ fetchí•  ë•Œ ì™œ fileì´ë‚˜ Environment variableë¡œ applicationì´ ê°€ì ¸ì˜¤ëŠ”ì§€?
- remote storageë¥¼ ì„¤ì •í•  ë•Œ PersistentVolumeê³¼ PersistentVolumeClaimìœ¼ë¡œ Abstractionì„ ì–´ë–»ê²Œ í•˜ëŠ”ì§€?
- pod, node objectë„ day 0ì—ì„œëŠ” CRD(Custom Resource Definition)ì˜ ì¤‘ í•˜ë‚˜ë¼ëŠ” ê²ƒì˜ ì˜ë¯¸.

Kubernetes Principles
- Kube API declarative over imperative
- No hidden internal APIs
- Meet the user where they are
- Workload portability
